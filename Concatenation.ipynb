{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outil de concaténation et de classification par cohortes des courbes de charge\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des pods de chaque cohorte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheminC=input(\"Chemin de l'excel contenant les numéros des pods de chaque cohorte : \")\n",
    "Chemin_Cohortes=r\"{}\".format(CheminC)\n",
    "Noms_Cohortes=[\"LE CEDRE\",\"CHANEL\",\"Toulouse lot 5\",\"AIH\",\"St-Etienne\",\"Ze Plug AO\"]\n",
    "Cohortes=pd.read_excel(Chemin_Cohortes,sheet_name='AO',usecols=Noms_Cohortes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des chemins des fichiers csv d'ENEDIS. Il faut renseigner le chemin du dossier avec toutes les courbes de charge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers dans le dossier = 2983\n",
      "Nombre de fichiers csv d'ENEDIS pris = 2962\n"
     ]
    }
   ],
   "source": [
    "CheminD=input(\"Chemin du dossier contenant les csv des courbes de charge : \")\n",
    "Chemin_dossier_CDC=r\"{}\".format(CheminD)\n",
    "chemins = []\n",
    "nb=0\n",
    "for racine, dirs, fichiers in os.walk(f\"{Chemin_dossier_CDC}\"):\n",
    "    for file in fichiers:\n",
    "        nb+=1\n",
    "#La condition if permet de ne choisir que les fichiers csv qui commencent par Enedis et dont la taille est supérieure à 19ko\n",
    "        if file.endswith(\".csv\") and file.startswith(\"Enedis\") and os.path.getsize(racine+\"\\\\\"+file)>=19000:\n",
    "            s = os.path.join(racine, file)\n",
    "            chemins.append(s)\n",
    "print(f\"Nombre de fichiers dans le dossier = {nb}\\nNombre de fichiers csv d'ENEDIS pris = {len(chemins)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification des CdC selon leur cohorte et supression des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listes_chemins_cohortes(chemins,Cohortes):\n",
    "    l1,l2,l3,l4,l5,l6,lnone,l,luniques,ldoublons=[],[],[],[],[],[],[],[],[],[]\n",
    "    seen=set()\n",
    "\n",
    "# création d'une liste de tuples formés des chemins csv des PODs et de leur numéro\n",
    "    for chemin_csv in chemins:\n",
    "        num_pod=pd.read_csv(chemin_csv,sep=';',nrows=1,usecols=[0]).loc[0,\"Identifiant PRM\"]\n",
    "        l.append((chemin_csv,int(num_pod)))\n",
    "\n",
    "#création de deux listes : luniques est la liste des tuples sans doublons \n",
    "#et ldoublons est la liste des tuples qui étaient doublons dans la liste l et qui ne sont pas dans luniques\n",
    "    for tup in l:\n",
    "        if tup[1] not in seen:\n",
    "            seen.add(tup[1])\n",
    "            luniques.append(tup)\n",
    "        else:\n",
    "            ldoublons.append(tup)\n",
    "\n",
    "#Classification des tuples uniques par cohorte            \n",
    "    for ch in luniques:\n",
    "        if ch[1] in Cohortes['LE CEDRE'].tolist():\n",
    "            l1.append((ch[0],ch[1]))\n",
    "        elif ch[1] in Cohortes['CHANEL'].tolist():\n",
    "            l2.append((ch[0],ch[1]))\n",
    "        elif ch[1] in Cohortes['Toulouse lot 5'].tolist():\n",
    "            l3.append((ch[0],ch[1]))\n",
    "        elif ch[1] in Cohortes['AIH'].tolist():\n",
    "            l4.append((ch[0],ch[1]))\n",
    "        elif ch[1] in Cohortes['St-Etienne'].tolist():\n",
    "            l5.append((ch[0],ch[1]))\n",
    "        elif ch[1] in Cohortes['Ze Plug AO'].tolist():\n",
    "            l6.append((ch[0],ch[1]))\n",
    "        else :\n",
    "            lnone.append((ch[0],ch[1]))\n",
    "\n",
    "    return ldoublons,l1,l2,l3,l4,l5,l6,lnone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "liste_doublons,l_LECEDRE,l_CHANEL,l_Tlse,l_AIH,l_StEtienne,l_Ze,lnone=listes_chemins_cohortes(chemins,Cohortes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concaténation, rééchantillonnage temporel et sommation pour chaque cohorte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concaténationPODS(lchemins):\n",
    "    #Création d'un dataframe vide dfc avec un dateTimeindex allant de nov 2020 à fev 2023 avec un pas de 30 minutes\n",
    "    dfc=pd.DataFrame(index=pd.date_range(start=pd.Timestamp('2020-11-11 00:00:00', tz='Europe/Paris'),\n",
    "                                         end=pd.Timestamp('2023-02-17 00:00:00', tz='Europe/Paris'),freq='30T'))\n",
    "    \n",
    "    # Importation des données de chaque POD de la cohorte et concaténation dans le dataframe dfc\n",
    "    for path,num in lchemins:\n",
    "        df = pd.read_csv(path, sep=';', skiprows=2, usecols=['Horodate', 'Valeur']) \n",
    "        df = df.set_index('Horodate') \n",
    "        df.index = pd.to_datetime(df.index,utc=True).tz_convert('Europe/Paris')\n",
    "        df = df.resample('30T').mean() # rééchantillonnage au pas demi-horaire de l'horizon temporel\n",
    "        dfc = pd.concat([dfc, df.rename(columns={'Valeur': num})], axis=1) #Ajout au dataframe vide la colonne des valeur du POD\n",
    "\n",
    "    #renvoi de la somme de toutes les valeurs des PODs de la cohorte pour chaque pas demi-horaire\n",
    "    return dfc.sum(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfLECEDRE= concaténationPODS(l_LECEDRE)\n",
    "dfAIH=concaténationPODS(l_AIH)\n",
    "dfCHANEL=concaténationPODS(l_CHANEL)\n",
    "dfStEtienne=concaténationPODS(l_StEtienne)\n",
    "dfTlse=concaténationPODS(l_Tlse)\n",
    "dfZe=concaténationPODS(l_Ze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfnone=concaténationPODS(lnone)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concaténation et exportations des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un dataframe contenant les valeurs de consommation au pas demi horaire classées par cohorte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldataframes=[dfLECEDRE,dfCHANEL,dfTlse,dfAIH,dfStEtienne,dfZe,dfnone]\n",
    "dftot=pd.concat(ldataframes,axis=1,verify_integrity=True) #agrégation des dataframes\n",
    "dftot=dftot.rename(columns={i:Noms_Cohortes[i] for i in range(len(Noms_Cohortes))})\n",
    "dftot=dftot.rename(columns={6:\"Autre\"})\n",
    "dftot.index=dftot.index.tz_localize(None) #conversion des dates au format tz-naive car excel ne prend pas en charge le format tz-aware"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un dataframe contenant les numéros et les chemins des PODs doublons supprimés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdoublons=pd.DataFrame({\"N° PODS en doubles supprimés\":[i[1] for i in liste_doublons],\"chemins\":[i[0] for i in liste_doublons]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un excel avec une feuille contenant dftot et une feuille contenant dfdoublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with pd.ExcelWriter(Chemin_dossier_CDC+\"\\\\CdC PODs jaunes.xlsx\") as writer:\n",
    "    dftot.to_excel(writer,sheet_name=\"CdC\", startrow=1)\n",
    "    dfdoublons.to_excel(writer,sheet_name=\"Doublons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ca7da86a5746769277311321674abaf2fc79cfd8d8ef3caf2e68cf5b6e6d43b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
