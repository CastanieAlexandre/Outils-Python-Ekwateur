{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outil de concaténation et de classification par cohortes des courbes de charge\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des chemins des fichiers csv d'ENEDIS. Il faut renseigner le chemin du dossier avec toutes les courbes de charge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers dans le dossier = 6044\n",
      "Nombre de fichiers csv d'ENEDIS pris = 5944\n"
     ]
    }
   ],
   "source": [
    "CheminD=input(\"Chemin du dossier contenant les csv des courbes de charge : \")\n",
    "def import_fichiers(CheminD):\n",
    "    Chemin_dossier_CDC=r\"{}\".format(CheminD)\n",
    "    chemins = []\n",
    "    nb=0\n",
    "    for racine, dirs, fichiers in os.walk(f\"{Chemin_dossier_CDC}\"):\n",
    "        for file in fichiers:\n",
    "            nb+=1\n",
    "    #La condition if permet de ne choisir que les fichiers csv qui commencent par Enedis et dont la taille est supérieure à 19ko\n",
    "            if file.endswith(\".csv\") and file.startswith(\"Enedis\") and os.path.getsize(racine+\"\\\\\"+file)>=19000 :\n",
    "                s = os.path.join(racine, file)\n",
    "                # dfch=pd.read_csv(s, sep=';', skiprows=2, usecols=['Horodate'])\n",
    "                # dfch=dfch.set_index('Horodate') \n",
    "                # dfch.index = pd.to_datetime(dfch.index,utc=True).tz_convert('Europe/Paris')\n",
    "                # if pd.to_datetime(\"2023-01-01\").tz_localize('Europe/Paris') and pd.to_datetime(\"2023-01-20\").tz_localize('Europe/Paris') in dfch.index:\n",
    "                chemins.append(s)\n",
    "    return nb,chemins\n",
    "\n",
    "nb,chemins=import_fichiers(CheminD)\n",
    "print(f\"Nombre de fichiers dans le dossier = {nb}\\nNombre de fichiers csv d'ENEDIS pris = {len(chemins)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supression des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listes_doublons(chemins):\n",
    "    l,luniques,ldoublons=[],[],[]\n",
    "    seen=set()\n",
    "\n",
    "# création d'une liste de tuples formés des chemins csv des PODs et de leur numéro\n",
    "    for chemin_csv in chemins:\n",
    "        num_pod=pd.read_csv(chemin_csv,sep=';',nrows=1,usecols=[0]).loc[0,\"Identifiant PRM\"]\n",
    "        l.append((chemin_csv,int(num_pod)))\n",
    "\n",
    "#création de deux listes : luniques est la liste des tuples sans doublons \n",
    "#et ldoublons est la liste des tuples qui étaient doublons dans la liste l et qui ne sont pas dans luniques\n",
    "    for tup in l:\n",
    "        if tup[1] not in seen:\n",
    "            seen.add(tup[1])\n",
    "            luniques.append(tup)\n",
    "        else:\n",
    "            ldoublons.append(tup)\n",
    "\n",
    "    return ldoublons,luniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "liste_doublons,liste_pods=listes_doublons(chemins)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concaténation, rééchantillonnage temporel et sommation pour chaque cohorte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concaténationPODS(lchemins):\n",
    "    #Création d'un dataframe vide dfc avec un dateTimeindex allant de nov 2020 à fev 2023 avec un pas de 30 minutes\n",
    "    dfc=pd.DataFrame(index=pd.date_range(start=pd.Timestamp('2023-01-01 00:00:00', tz='Europe/Paris'),\n",
    "                                         end=pd.Timestamp('2023-01-21 00:00:00', tz='Europe/Paris'),freq='D'))\n",
    "    \n",
    "    # Importation des données de chaque POD de la cohorte et concaténation dans le dataframe dfc\n",
    "    for path,num in lchemins:\n",
    "        df = pd.read_csv(path, sep=';', skiprows=2, usecols=['Horodate', 'Valeur']) \n",
    "        df = df.set_index('Horodate') \n",
    "        df.index = pd.to_datetime(df.index,utc=True).tz_convert('Europe/Paris')\n",
    "        df = df.resample('D').mean() # rééchantillonnage au pas demi-horaire de l'horizon temporel\n",
    "        dfc = pd.concat([dfc, df.rename(columns={'Valeur': num})], axis=1) #Ajout au dataframe vide la colonne des valeur du POD\n",
    "    return dfc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dftot= concaténationPODS(liste_pods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftot.index=dftot.index.tz_localize(None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concaténation et exportations des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un dataframe contenant les numéros et les chemins des PODs doublons supprimés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdoublons=pd.DataFrame({\"N° PODS en doubles supprimés\":[i[1] for i in liste_doublons],\"chemins\":[i[0] for i in liste_doublons]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un excel avec une feuille contenant dftot et une feuille contenant dfdoublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with pd.ExcelWriter(Chemin_dossier_CDC+\"\\\\CdC PODs jaunes.xlsx\") as writer:\n",
    "    dftot.to_excel(writer,sheet_name=\"CdC\", startrow=1)\n",
    "    dfdoublons.to_excel(writer,sheet_name=\"Doublons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ca7da86a5746769277311321674abaf2fc79cfd8d8ef3caf2e68cf5b6e6d43b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
